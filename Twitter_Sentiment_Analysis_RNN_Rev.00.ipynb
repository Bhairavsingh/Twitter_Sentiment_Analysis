{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Twitter_Sentiment_Analysis_RNN_Rev.00.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1trPBFB9ToMx1JcsMfCe73W6rZsan7HsD","authorship_tag":"ABX9TyP+1cnAjVsKUqTf+KtDOcjt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"CxlzCRWNe5-6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597782412008,"user_tz":300,"elapsed":3855,"user":{"displayName":"Bhairavsingh Ghorpade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh386l9JKpmLmwBGEG1QJr6Rz70jGkNK9kKIpx4=s64","userId":"05779038079998438907"}}},"source":["\n","#Loading required libraries.\n","import pandas as pd\n","import numpy as np\n","import re\n","import string\n","from pprint import pprint\n","from collections import Counter\n","import keras\n","\n","from keras.layers import Embedding\n","import tensorflow as tf\n","\n","from keras.datasets import imdb\n","from keras import preprocessing\n","\n","\n","\n","#Loading nltk library and its packages for text processing.\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn import metrics\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","\n","#Downloading some necessary word lists.\n","#nltk.download('wordnet')\n","#nltk.download('stopwords')\n","\n","#Downloading all dependencies and corpora.\n","#nltk.download()\n","#nltk.download('all', halt_on_error=False)\n","\n","#Importing pattern and its dependencies.\n","#import pattern\n","#from pattern.en import tag\n","#from pattern.en import parse\n","#from pattern.en import pprint\n","\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFtLbdfjfFvh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597782582650,"user_tz":300,"elapsed":4474,"user":{"displayName":"Bhairavsingh Ghorpade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh386l9JKpmLmwBGEG1QJr6Rz70jGkNK9kKIpx4=s64","userId":"05779038079998438907"}}},"source":["#Reading Training and Testing Datasets.\n","raw_data = pd.read_csv(\"/content/drive/My Drive/Colab_Datasets/Tweets_Data/train.csv\")\n","#test_data = pd.read_csv(\"C:/Users/Bhair/OneDrive - University of Oklahoma/Master_of_Science_Data/Data Science Practice/Twitter Sentiment Analysis/Data/tweet-sentiment-extraction/test.csv\")\n","\n","\n","\n","#Contraction directory.\n","\n","CONTRACTION_MAP = {\n","\"ain't\": \"is not\",\n","\"aren't\": \"are not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he will\",\n","\"he'll've\": \"he he will have\",\n","\"he's\": \"he is\",\n","\"how'd\": \"how did\",\n","\"how'd'y\": \"how do you\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how is\",\n","\"I'd\": \"I would\",\n","\"I'd've\": \"I would have\",\n","\"I'll\": \"I will\",\n","\"I'll've\": \"I will have\",\n","\"I'm\": \"I am\",\n","\"I've\": \"I have\",\n","\"i'd\": \"i would\",\n","\"i'd've\": \"i would have\",\n","\"i'll\": \"i will\",\n","\"i'll've\": \"i will have\",\n","\"i'm\": \"i am\",\n","\"i've\": \"i have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it would\",\n","\"it'd've\": \"it would have\",\n","\"it'll\": \"it will\",\n","\"it'll've\": \"it will have\",\n","\"it's\": \"it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"mightn't've\": \"might not have\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"mustn't've\": \"must not have\",\n","\"needn't\": \"need not\",\n","\"needn't've\": \"need not have\",\n","\"o'clock\": \"of the clock\",\n","\"oughtn't\": \"ought not\",\n","\"oughtn't've\": \"ought not have\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"shan't've\": \"shall not have\",\n","\"she'd\": \"she would\",\n","\"she'd've\": \"she would have\",\n","\"she'll\": \"she will\",\n","\"she'll've\": \"she will have\",\n","\"she's\": \"she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"shouldn't've\": \"should not have\",\n","\"so've\": \"so have\",\n","\"so's\": \"so as\",\n","\"that'd\": \"that would\",\n","\"that'd've\": \"that would have\",\n","\"that's\": \"that is\",\n","\"there'd\": \"there would\",\n","\"there'd've\": \"there would have\",\n","\"there's\": \"there is\",\n","\"they'd\": \"they would\",\n","\"they'd've\": \"they would have\",\n","\"they'll\": \"they will\",\n","\"they'll've\": \"they will have\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"to've\": \"to have\",\n","\"wasn't\": \"was not\",\n","\"we'd\": \"we would\",\n","\"we'd've\": \"we would have\",\n","\"we'll\": \"we will\",\n","\"we'll've\": \"we will have\",\n","\"we're\": \"we are\",\n","\"we've\": \"we have\",\n","\"weren't\": \"were not\",\n","\"what'll\": \"what will\",\n","\"what'll've\": \"what will have\",\n","\"what're\": \"what are\",\n","\"what's\": \"what is\",\n","\"what've\": \"what have\",\n","\"when's\": \"when is\",\n","\"when've\": \"when have\",\n","\"where'd\": \"where did\",\n","\"where's\": \"where is\",\n","\"where've\": \"where have\",\n","\"who'll\": \"who will\",\n","\"who'll've\": \"who will have\",\n","\"who's\": \"who is\",\n","\"who've\": \"who have\",\n","\"why's\": \"why is\",\n","\"why've\": \"why have\",\n","\"will've\": \"will have\",\n","\"won't\": \"will not\",\n","\"won't've\": \"will not have\",\n","\"would've\": \"would have\",\n","\"wouldn't\": \"would not\",\n","\"wouldn't've\": \"would not have\",\n","\"y'all\": \"you all\",\n","\"y'all'd\": \"you all would\",\n","\"y'all'd've\": \"you all would have\",\n","\"y'all're\": \"you all are\",\n","\"y'all've\": \"you all have\",\n","\"you'd\": \"you would\",\n","\"you'd've\": \"you would have\",\n","\"you'll\": \"you will\",\n","\"you'll've\": \"you will have\",\n","\"you're\": \"you are\",\n","\"you've\": \"you have\"\n","}\n","\n","\n","\n","#Customized stop words.\n","stop_words = {'a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', \n","              'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \n","              'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'he', 'her', 'here', \n","              'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'it', \"it's\", 'its', \n","              'itself', 'just', 'm', 'ma', 'me', 'more', 'most', 'my', 'myself', 'now', 'o', 'of', 'off', 'on', \n","              'once', 'only', 'or', 'other', 'our',  'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', \n","              'shan', \"shan't\", 'she', \"she's\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', \n","              'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', \n","              'too', 'under', 'until', 'up', 've', 'very', 'we', 'while', 'with', 'y', 'you', \"you'd\", \"you'll\", \n","              \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves'}\n","\n","\n","\n","#Function for expanding contractions using above dictonary of contractions.\n","def expand_contractions(sentence, contraction_mapping):\n","    #Creating a list of contraction keys.\n","    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n","                                      flags = re.IGNORECASE|re.DOTALL)\n","    \n","    #Function for expanding the contractions.\n","    def expand_match(contraction):\n","        match = contraction.group(0)\n","        first_char = match[0]\n","        expanded_contraction = (contraction_mapping.get(match)\\\n","                                if contraction_mapping.get(match)\\\n","                                else contraction_mapping.get(match.lower()))\n","        expanded_contraction = first_char + expanded_contraction[1:]\n","        return expanded_contraction\n","    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n","    return expanded_sentence\n","\n","\n","\n","#Function for creating a list ofstrings from the input. It creates different elements in the list if there is a space.\n","#Basically, a sentence will be converted into a list of words.\n","def words(text): \n","  return re.findall(r'\\w+', text.lower())\n","\n","#Reading the file with vocabulary.\n","WORDS = Counter(words(open('/content/drive/My Drive/Colab_Datasets/Tweets_Data/big.txt').read()))\n","\n","#Function for calculating probability of the given word to find in the vocabulary.\n","def P(word, N = sum(WORDS.values())): \n","    \"Probability of `word`.\"\n","    #WORDS[word] gives the count of word\n","    #N is total number of words in the vocabulary.\n","    return WORDS[word] / N\n","\n","#Main function which should be called for word correction.\n","def correction(word): \n","    \"Most probable spelling correction for word.\"\n","    return max(candidates(word), key = P)\n","\n","#Function for genetraing all possible real words from given string.\n","def candidates(word): \n","    \"Generate possible spelling corrections for word.\"\n","    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n","\n","def known(words): \n","    \"The subset of `words` that appear in the dictionary of WORDS.\"\n","    return set(w for w in words if w in WORDS)\n","\n","#Function for editing given string. This returns words which are one edit away from input string.\n","#The function edits1 returns a set of all the edited strings (whether words or not) that can be made with one simple edit: \n","#a simple edit to a word is a deletion (remove one letter), a transposition (swap two adjacent letters), a replacement (change one letter to another) or an insertion (add a letter).\n","#The output of this function can be a big set. For a word of length n, there will be n deletions, n-1 transpositions, \n","#26n alterations, and 26(n+1) insertions, for a total of 54n+25 (of which a few are typically duplicates)\n","def edits1(word):\n","    \"All edits that are one edit away from `word`.\"\n","    letters    = 'abcdefghijklmnopqrstuvwxyz'\n","    #Splitting the letters of a word to form a combination.\n","    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","    deletes    = [L + R[1:]               for L, R in splits if R]\n","    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","    inserts    = [L + c + R               for L, R in splits for c in letters]\n","    return set(deletes + transposes + replaces + inserts)\n","\n","#Function for generating set of words which are two edits away from input string.\n","def edits2(word): \n","    \"All edits that are two edits away from `word`.\"\n","    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n","\n","\n","#Function for converting a list to a string.\n","def listToString(inlist):\n","    #Initialize an empty string.\n","    string = \" \"\n","    return (string.join(inlist))\n","\n","\n","\n","\n","#Function for preprocessing and normalizing text data.\n","def data_normalization(indata):\n","    #Setting stop word variable using NLTK package.\n","    stopWords = set(stopwords.words('english'))\n","    \n","    #Creating list of punctuations.\n","    punctuations = [\".\", \",\", \"!\", \"`\", \"...\", \"****\", \":\", \"(\", \")\", \"?\", \"-\", \"{\", \"}\", \"[\", \"]\", \";\", \"/\", \"+\", \"<\", \n","               \">\", \"|\", \"=\" \"_\", \"^\", \"@\", \"#\", \"$\", \"%\", \"~\", \"'\", \"..\"]\n","    \n","    #Regular expression tokenizer with only alphanumeric contents.\n","    Token_Pattern = r'\\w+'\n","    regex_wt = nltk.RegexpTokenizer(pattern = Token_Pattern, gaps = False)\n","\n","    #Listing out all contractions.\n","    contaction_keys = list(CONTRACTION_MAP.keys())\n","\n","    #Declaring list for storing lemmatized tweets.\n","    lemmatized_words = []\n","    for i in range(len(indata)):\n","        #Condition for empty tweets.\n","        if (str(indata[i]) == \"nan\"):\n","            #Fake strink in the blank tweet.\n","            indata[i] = \"NA\"\n","        #Expanding contractions.\n","        text_segment = indata[i].replace(\"`\", \"'\")\n","        expanded_tweets = expand_contractions(text_segment, CONTRACTION_MAP)\n","        #Removing URL links from tweets.\n","        expanded_tweets = re.sub(r'http\\S+', '', expanded_tweets, flags = re.MULTILINE)\n","        #print(\"After contractions: \", expanded_tweets)\n","        #Tokenizing the words.\n","        tokens = regex_wt.tokenize(expanded_tweets)\n","        #Declaring list for lemmatized words.\n","        lemm_list = []\n","        for word in tokens:\n","            #Condition for stopwords.\n","            #if (word not in stopWords):\n","            if (word not in stop_words):\n","                #Condition for punctuations.\n","                if (word not in punctuations):\n","                    #Correcting the word.\n","                    #Not using this function because its not correcting words properly. Need to find more accurate way!\n","                    #corrected_spell = correction(word)\n","                    corrected_spell = word\n","                    #Converting all letters to lower form.\n","                    corrected_spell = corrected_spell.lower()\n","                    #Lemmatizing words.\n","                    lemm_list.append(wordnet_lemmatizer.lemmatize(corrected_spell, pos = \"v\"))\n","        #Converting the lemmatized word's (document) list to a string.\n","        lemm_string = listToString(lemm_list)\n","        lemmatized_words.append(lemm_string)\n","    return lemmatized_words\n","\n","\n","#Function for extracting features from given data. There are three options of extraction methods are given:\n","#Those are Bag of Words-based frequencies, occurrences, and TF-IDF based features\n","def build_feature_matrix(documents, feature_type='frequency', ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n","    feature_type = feature_type.lower().strip()\n","    if feature_type == 'binary':\n","        vectorizer = CountVectorizer(binary=True, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n","    \n","    elif feature_type == 'frequency':\n","        vectorizer = CountVectorizer(binary=False, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n","    \n","    elif feature_type == 'tfidf':\n","        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n","    \n","    else:\n","        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n","    \n","    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n","    return (vectorizer, feature_matrix)\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmAwiZZqcnlJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1597782594683,"user_tz":300,"elapsed":539,"user":{"displayName":"Bhairavsingh Ghorpade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh386l9JKpmLmwBGEG1QJr6Rz70jGkNK9kKIpx4=s64","userId":"05779038079998438907"}},"outputId":"a575862a-7b58-46bb-8c1d-ef5f61dc2e71"},"source":["#Normalizing tweets.\n","normalized_tweets = data_normalization(raw_data['text'])"],"execution_count":4,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-3ebe83c5998d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Normalizing tweets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnormalized_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-c05b17a35761>\u001b[0m in \u001b[0;36mdata_normalization\u001b[0;34m(indata)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_normalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m#Setting stop word variable using NLTK package.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mstopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m#Creating list of punctuations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"]}]},{"cell_type":"code","metadata":{"id":"ytakqU01fF8Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1597668766738,"user_tz":300,"elapsed":472,"user":{"displayName":"Bhairavsingh Ghorpade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh386l9JKpmLmwBGEG1QJr6Rz70jGkNK9kKIpx4=s64","userId":"05779038079998438907"}},"outputId":"26549aca-b42e-45d0-cebe-a20c9f442386"},"source":["\n","print(raw_data['text'][0])\n","print(normalized_tweets[:2])\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":[" I`d have responded, if I were going\n","['i would have respond i be go', 'sooo sad i will miss san diego']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vUceJW1LfF-T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1597668782263,"user_tz":300,"elapsed":421,"user":{"displayName":"Bhairavsingh Ghorpade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh386l9JKpmLmwBGEG1QJr6Rz70jGkNK9kKIpx4=s64","userId":"05779038079998438907"}},"outputId":"d6923f63-b304-4c9e-885c-043fee1c2d6b"},"source":["documents = np.array(normalized_tweets[:2])\n","documents\n","\n","sample_vectorizer, sample_feature_matrix = build_feature_matrix(documents, feature_type = 'tfidf', \n","                                                                ngram_range = (1, 1), min_df = 0.0, max_df = 1.0)\n","\n","#print(sample_vectorizer)\n","print(sample_feature_matrix[0])"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['i would have respond i be go', 'sooo sad i will miss san diego'],\n","      dtype='<U30')"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"pouSMU4SfGLh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597684012026,"user_tz":300,"elapsed":775,"user":{"displayName":"Bhairavsingh Ghorpade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh386l9JKpmLmwBGEG1QJr6Rz70jGkNK9kKIpx4=s64","userId":"05779038079998438907"}},"outputId":"7a49fa3a-83f6-4f1a-8b36-cb5d7277a34f"},"source":["\n","embedding_layer = Embedding(1000, 64)\n"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff1e1d0a470>"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"6nFMOkLoYXc4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597685408080,"user_tz":300,"elapsed":5781,"user":{"displayName":"Bhairavsingh Ghorpade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh386l9JKpmLmwBGEG1QJr6Rz70jGkNK9kKIpx4=s64","userId":"05779038079998438907"}}},"source":["\n","#Number of words considered for constructing features.\n","max_features = 10000\n","#Curring off the text after given number of words.\n","maxlen = 20\n","\n","#Loading data.\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n","\n","#Converting the integers into a 2D integer tensor of shape (number of samples, max length)\n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = maxlen)\n"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"X7U25IOoYX19","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":246},"executionInfo":{"status":"error","timestamp":1597782391349,"user_tz":300,"elapsed":444,"user":{"displayName":"Bhairavsingh Ghorpade","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh386l9JKpmLmwBGEG1QJr6Rz70jGkNK9kKIpx4=s64","userId":"05779038079998438907"}},"outputId":"79d86440-c3f9-4158-e06c-58103583dc80"},"source":["\n","import os\n","\n","imdb_dir = '/Users/Bhair/Downloads/aclImdb'\n","#C:\\Users\\Bhair\\Downloads\\aclImdb\n","train_dir = os.path.join(imdb_dir, 'train')\n","labels = []\n","texts = []\n","\n","for label_type in ['neg', 'pos']:\n","  dir_name = os.path.join(train_dir, label_type)\n","  for fname in os.listdir(dir_name):\n","    if fname[-4:] == '.txt':\n","      f = open(os.path.join(dir_name, fname))\n","      texts.append(f.read())\n","      f.close()\n","      \n","      if label_type == 'neg':\n","        labels.append(0)\n","      else:\n","        labels.append(1)\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-87c276786db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mdir_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/Bhair/Downloads/aclImdb/train/neg'"]}]},{"cell_type":"code","metadata":{"id":"rZ0mI-cGYX4m","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pgn_mRK6YX9H","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mLCfA2o5YYBz","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9MlDFmfYYFE","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"goeViKhnYYHm","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6oKpjYVYYK4","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"InpgpuNcYYOe","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlDABTx2YYQ_","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYSm_BTVYYAr","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}